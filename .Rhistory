result=origami_SuperLearner(folds=folds,X=Wmat,Y=A, SL.library=SL.library,
family=list(family="multinomial"), method=method.mnNNloglik())
print(result)
SL.library=c("mnSL.glmnet", "mnSL.multinom", "mnSL.mean", "mnSL.polymars","mnSL.gbm","mnSL.randomForest")
folds=make_folds(n,V=10)
result=origami_SuperLearner(folds=folds,X=Wmat,Y=A, SL.library=SL.library,
family=list(family="multinomial"), method=method.mnNNloglik())
result
# test=method.mnNNloglik()
# test$computeCoef(result$Z,result$valY,SL.library,T,result$valWeights)
# Z=result$Z
# Y=result$valY
# libraryNames=SL.library
# obsWeights=result$valWeights
####
# plot results
#get true probabilities
Wseq=seq(from=-4,to=4,length=1000)
true=g0(Wseq)
trueg=data.frame(W=Wseq,true)
trueglong=melt(trueg,id="W")
#get predictions
preds=predict(result,newdata=Wmat)$pred
colnames(preds)=colnames(true)
predg=data.frame(W=W,preds)
predglong=melt(predg,id="W")
#plot true and predicted
ggplot(predglong,aes(x=W,y=value,color=variable))+geom_point()+geom_line(data=trueglong)+theme_bw()+ylab("p(A)")
setwd("~/Dropbox/origami/")
library(devtools)
document()
build()
install()
library(origami)
library(reshape2)
library(ggplot2)
####
# generate test data
g0=function(W) {
# rep(0.5, nrow(W))
scale_factor=0.8
A1=plogis(scale_factor * (W-1))
A2=plogis(scale_factor * (-W-1))
A3=plogis(1-1*scale_factor * W^2)
A=cbind(A1, A2, A3)
# make sure A sums to 1
A=normalize_rows(A)
}
set.seed(1234)
n=1000
W=rnorm(n)
W2=rnorm(n) #package is buggy if there's only one W
Wmat=cbind(W,W2)
g0W=g0(W)
A=factor(apply(g0W, 1, function(pAi) which(rmultinom(1, 1, pAi) == 1)))
####
# run multinomial Super Learner
SL.library=c("mnSL.glmnet", "mnSL.multinom", "mnSL.mean", "mnSL.polymars","mnSL.gbm","mnSL.randomForest")
folds=make_folds(n,V=10)
result=origami_SuperLearner(folds=folds,X=Wmat,Y=A, SL.library=SL.library,
family=list(family="multinomial"), method=method.mnNNloglik())
# test=method.mnNNloglik()
# test$computeCoef(result$Z,result$valY,SL.library,T,result$valWeights)
# Z=result$Z
# Y=result$valY
# libraryNames=SL.library
# obsWeights=result$valWeights
####
# plot results
#get true probabilities
Wseq=seq(from=-4,to=4,length=1000)
true=g0(Wseq)
trueg=data.frame(W=Wseq,true)
trueglong=melt(trueg,id="W")
#get predictions
preds=predict(result,newdata=Wmat)$pred
colnames(preds)=colnames(true)
predg=data.frame(W=W,preds)
predglong=melt(predg,id="W")
#plot true and predicted
ggplot(predglong,aes(x=W,y=value,color=variable))+geom_point()+geom_line(data=trueglong)+theme_bw()+ylab("p(A)")
result
predict(result,newdata=Wmat)$library_preds
test=predict(result,newdata=Wmat)$library_preds
test
test=predict(result,newdata=Wmat)$library_pred
test
names(test)
dimnames(test,1)
dimnames(test)
test[,,5]
test[,,6]
test[,,6]==0
colSums(test[,,6]==0)
test[,,6]
rfpred=test[,,6]
libpreds=predict(result,newdata=Wmat)$library_pred
rfpred=test[,,6]
mn_loglik(rfpred,result$valY,result$valWeights)
origami:::mn_loglik(rfpred,result$valY,result$valWeights)
origami:::mn_loglik(rfpred,factor_to_indicators(result$valY),result$valWeights)
mn_loglik
origami:::mn_loglik
0*log(0)
log(0)
0^0
log(0^0)
1*log(0)
class_liks=truth * log(pred)
truth=result$valY
pred=rfpred
class_liks=truth * log(pred)
truth=factor_to_indicators(result$valY)
truth
class_liks=truth * log(pred)
class_liks
class_likes[is.nan(class_liks)]
class_liks[is.nan(class_liks)]
class_liks[truth==0]=0
class_liks[is.nan(class_liks)]
setwd("~/Dropbox/origami/")
library(devtools)
document()
build()
install()
library(origami)
library(reshape2)
library(ggplot2)
####
# generate test data
g0=function(W) {
# rep(0.5, nrow(W))
scale_factor=0.8
A1=plogis(scale_factor * (W-1))
A2=plogis(scale_factor * (-W-1))
A3=plogis(1-1*scale_factor * W^2)
A=cbind(A1, A2, A3)
# make sure A sums to 1
A=normalize_rows(A)
}
set.seed(1234)
n=1000
W=rnorm(n)
W2=rnorm(n) #package is buggy if there's only one W
Wmat=cbind(W,W2)
g0W=g0(W)
A=factor(apply(g0W, 1, function(pAi) which(rmultinom(1, 1, pAi) == 1)))
####
# run multinomial Super Learner
SL.library=c("mnSL.glmnet", "mnSL.multinom", "mnSL.mean", "mnSL.polymars","mnSL.gbm","mnSL.randomForest")
folds=make_folds(n,V=10)
result=origami_SuperLearner(folds=folds,X=Wmat,Y=A, SL.library=SL.library,
family=list(family="multinomial"), method=method.mnNNloglik())
print(result)
library(SuperLearner)
method.NNloglik
set.seed(1236)
n=1000
W=rnorm(n)
W2=rnorm(n) #package is buggy if there's only one W
Wmat=cbind(W,W2)
g0W=g0(W)
A=factor(apply(g0W, 1, function(pAi) which(rmultinom(1, 1, pAi) == 1)))
####
# run multinomial Super Learner
SL.library=c("mnSL.glmnet", "mnSL.multinom", "mnSL.mean", "mnSL.polymars","mnSL.gbm","mnSL.randomForest")
folds=make_folds(n,V=10)
result=origami_SuperLearner(folds=folds,X=Wmat,Y=A, SL.library=SL.library,
family=list(family="multinomial"), method=method.mnNNloglik())
print(result)
libpreds=predict(result,newdata=Wmat)$library_pred
rfpred=test[,,6]
rfpred=libpreds[,,6]
origami:::mn_loglik(rfpred,factor_to_indicators(result$valY),result$valWeights)
method=method.mnNNloglik()
method$computeCoef
Y_ind <- factor_to_indicators(result$valY)
cvRisk <- -2 * aaply(result$Z, 3, mn_loglik, Y_ind, result$obsWeights)
libpreds=result$Z
rfpred=libpreds[,,6]
method=method.mnNNloglik()
origami:::mn_loglik(rfpred,factor_to_indicators(result$valY),result$valWeights)
Y_ind <- factor_to_indicators(result$valY)
cvRisk <- -2 * aaply(result$Z, 3, mn_loglik, Y_ind, result$obsWeights)
cvRisk <- -2 * aaply(result$Z, 3, origami:::mn_loglik, Y_ind, result$obsWeights)
cvRisk <- -2 * aaply(result$Z, 3, origami:::mn_loglik, Y_ind, result$valWeights)
cvRisk
#get true probabilities
Wseq=seq(from=-4,to=4,length=1000)
true=g0(Wseq)
trueg=data.frame(W=Wseq,true)
trueglong=melt(trueg,id="W")
#get predictions
preds=predict(result,newdata=Wmat)$pred
colnames(preds)=colnames(true)
predg=data.frame(W=W,preds)
predglong=melt(predg,id="W")
#plot true and predicted
ggplot(predglong,aes(x=W,y=value,color=variable))+geom_point()+geom_line(data=trueglong)+theme_bw()+ylab("p(A)")
result=origami_SuperLearner(folds=folds,X=Wmat,Y=A, SL.library=SL.library,
family=list(family="multinomial"), method=method.mvSL(method.NNLS()))
traceback()
CV.SuperLearner
summary.CV.SuperLearner
?"origami_SuperLearner"
?CV.SuperLearner
set.seed(23432)
## training set
n <- 500
p <- 50
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)
# build Library and run Super Learner
SL.library <- c("SL.glm", "SL.randomForest", "SL.gam", "SL.polymars", "SL.mean")
## Not run:
test <- CV.SuperLearner(Y = Y, X = X, V = 10, SL.library = SL.library,
verbose = TRUE, method = "method.NNLS")
test
summary(test)
?SuperLearner
## Not run:
## simulate data
set.seed(23432)
## training set
n <- 500
p <- 50
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)
## test set
m <- 1000
newX <- matrix(rnorm(m*p), nrow = m, ncol = p)
colnames(newX) <- paste("X", 1:p, sep="")
newX <- data.frame(newX)
newY <- newX[, 1] + sqrt(abs(newX[, 2] * newX[, 3])) + newX[, 2] -
newX[, 3] + rnorm(m)
# generate Library and run Super Learner
SL.library <- c("SL.glm", "SL.randomForest", "SL.gam",
"SL.polymars", "SL.mean")
test <- SuperLearner(Y = Y, X = X, newX = newX, SL.library = SL.library,
verbose = TRUE, method = "method.NNLS")
test
summary(test)
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,
verbose = TRUE, method = "method.NNLS")
summary(test)
test
Y <- rbinom(1,1,plogis(X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)))
Y
?rbinom
Y <- rbinom(n,1,plogis(X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n))
)
Y
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial"
verbose = TRUE, method = "method.NNloglik")
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial",
verbose = TRUE, method = "method.NNloglik")
# generate Library and run Super Learner
SL.library <- c"SL.randomForest", "SL.mean")
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial",
verbose = TRUE, method = "method.NNloglik")
SL.library <- c("SL.randomForest", "SL.mean")
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial",
verbose = TRUE, method = "method.NNloglik")
test
Y <- rbinom(1,1,plogis(X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)))
## test set
m <- 50
newX <- matrix(rnorm(m*p), nrow = m, ncol = p)
colnames(newX) <- paste("X", 1:p, sep="")
newX <- data.frame(newX)
newY <- newX[, 1] + sqrt(abs(newX[, 2] * newX[, 3])) + newX[, 2] -
newX[, 3] + rnorm(m)
# generate Library and run Super Learner
SL.library <- c("SL.randomForest", "SL.mean")
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial",
verbose = TRUE, method = "method.NNloglik")
test
n <- 50
p <- 50
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- rbinom(1,1,plogis(X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)))
# generate Library and run Super Learner
SL.library <- c("SL.randomForest", "SL.mean")
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial",
verbose = TRUE, method = "method.NNloglik")
test
n <- 100
p <- 50
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- rbinom(1,1,plogis(X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)))
# generate Library and run Super Learner
SL.library <- c("SL.randomForest", "SL.mean")
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial",
verbose = TRUE, method = "method.NNloglik")
test
Y
Y <- rbinom(n,1,plogis(X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)))
Y
# generate Library and run Super Learner
SL.library <- c("SL.randomForest", "SL.mean")
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial",
verbose = TRUE, method = "method.NNloglik")
test
## Not run:
## simulate data
set.seed(23432)
## training set
n <- 100
p <- 50
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- rbinom(n,1,plogis(X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)))
# generate Library and run Super Learner
SL.library <- c("SL.randomForest", "SL.mean")
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial",
verbose = TRUE, method = "method.NNloglik")
test
## Not run:
## simulate data
set.seed(23432)
## training set
n <- 50
p <- 50
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- rbinom(n,1,plogis(X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)))
# generate Library and run Super Learner
SL.library <- c("SL.randomForest", "SL.mean")
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial",
verbose = TRUE, method = "method.NNloglik")
test
## Not run:
## simulate data
set.seed(23432)
## training set
n <- 50
p <- 50
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- rbinom(n,1,0.99)
# generate Library and run Super Learner
SL.library <- c("SL.randomForest", "SL.mean")
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial",
verbose = TRUE, method = "method.NNloglik")
test
## Not run:
## simulate data
set.seed(23432)
## training set
n <- 50
p <- 50
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- rbinom(n,1,0.90)
# generate Library and run Super Learner
SL.library <- c("SL.randomForest", "SL.mean")
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial",
verbose = TRUE, method = "method.NNloglik")
test
## Not run:
## simulate data
set.seed(23432)
## training set
n <- 50
p <- 50
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- rbinom(n,1,0.95)
# generate Library and run Super Learner
SL.library <- c("SL.randomForest", "SL.mean")
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial",
verbose = TRUE, method = "method.NNloglik")
test
## Not run:
## simulate data
set.seed(23432)
## training set
n <- 50
p <- 50
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- rbinom(n,1,0.95)
# generate Library and run Super Learner
SL.library <- c("SL.mean")
test <- SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial",
verbose = TRUE, method = "method.NNloglik")
test
## Not run:
## simulate data
set.seed(23432)
## training set
n <- 50
p <- 50
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- rbinom(n,1,0.95)
# generate Library and run Super Learner
SL.library <- c("SL.glm","SL.mean")
test <- CV.SuperLearner(Y = Y, X = X, SL.library = SL.library,family="binomial",
verbose = TRUE, method = "method.NNloglik")
test
test=read.csv("~/Downloads/trips.csv",stringsAsFactors=F)
names(test)
payload=test$current_payload[1]
parsed=fromJSON(payload)
library(jsonlite)
parsed=fromJSON(payload)
parsed
Raday:::meter_dump_to_dt(parsed)
library(Raday)
Raday:::meter_dump_to_dt(parsed)
parse_trip_payload=function(payload){
parsed=fromJSON(payload)
Raday:::meter_dump_to_dt(parsed)
}
current_loads=rbindlist(lapply(test$current_payload,parse_trip_payload))
table(is.null(test$current_payload))
names(test)
dim(test)
table(is.null(test$current_payload),exclude=c())
table(is.na(test$current_payload))
table((test$current_payload)=="")
trips=read.csv("~/Downloads/trips.csv",stringsAsFactors=F)
trips=as.data.table(trips)
trips=trips[current_payload!=""]
parse_trip_payload=function(payload){
parsed=fromJSON(payload)
Raday:::meter_dump_to_dt(parsed)
}
current_loads=rbindlist(lapply(test$current_payload,parse_trip_payload))
dim(trips)
payload=trips$current_payload[1]
parsed=fromJSON(payload)
Raday:::meter_dump_to_dt(parsed)
test=Raday:::meter_dump_to_dt(parsed)
test
current_loads=rbindlist(lapply(trips$current_payload,parse_trip_payload))
current_loads
prev_loads=rbindlist(lapply(trips$previous_payload,parse_trip_payload))
dim(current_loads)
dim(prev_loads)
current_waves <- data.table:::melt.data.table(current_loads, id = c("hardware_id", "time", "index"), measure = Raday:::NICE_COLS, variable.name = "channel")
prev_waves <- data.table:::melt.data.table(prev_loads, id = c("hardware_id", "time", "index"), measure = Raday:::NICE_COLS, variable.name = "channel")
prev_waves
current_waves[,last_value:=prev_waves$value]
current_waves[,diff:=value-last_value]
rmsdiffs=current_waves[,rms(diff),by=list(hardware_id,time,channel)]
rmsdiffs=current_waves[,list(rmsdiff=rms(diff)),by=list(hardware_id,time,channel)]
quantile(rmsdiffs[channel=="voltage",rmsdiff]
)
ggplot(current_waves[channel=="voltage"],aes(x=index,y=value,color=hardware_id))+geom_line()+facet_wrap(~time)
ggplot(current_waves[channel=="voltage"],aes(x=index,y=value,color=hardware_id))+geom_line()+facet_wrap(~time,scales="free")
sub=current_waves[channel=="voltage"&time>as.POSIXct("2015-11-18",tz="UTC")]
ggplot(sub,aes(x=index,y=value,color=hardware_id))+geom_line()+facet_wrap(~time,scales="free")
ggplot(sub,aes(x=index,y=value))+geom_line()+facet_wrap(~hardware_id,scales="free")
ggplot(sub,aes(x=index,y=value))+geom_line()+facet_wrap(~time+hardware_id,scales="free")
ggplot(sub,aes(x=index,y=value))+geom_line()+geom_line(aes(y=last_value),color="red")+facet_wrap(~time+hardware_id,scales="free")
ggplot(sub,aes(x=index,y=value))+geom_line()+geom_line(aes(y=last_value),color="red")+facet_grid(time~hardware_id,scales="free")
0.1*120
library(formatR)
?tidy_dir
?CV.SuperLearner
xamples
set.seed(23432)
## training set
n <- 500
p <- 50
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)
# build Library and run Super Learner
SL.library <- c("SL.glm", "SL.randomForest", "SL.gam", "SL.polymars", "SL.mean")
## Not run:
test <- CV.SuperLearner(Y = Y, X = X, V = 10, SL.library = SL.library,
verbose = TRUE, method = "method.NNLS")
xamples
set.seed(23432)
## training set
n <- 500
p <- 50
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)
# build Library and run Super Learner
SL.library <- c("SL.glm", "SL.mean")
## Not run:
test <- CV.SuperLearner(Y = Y, X = X, V = 10, SL.library = SL.library,
verbose = TRUE, method = "method.NNLS")
test
summary(test)
SL.predict(test)
predict(test)
library(origami)
?cross_validae
?cross_validate
