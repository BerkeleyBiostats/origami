last_event_timestamp=run_query("MachineLearning","SELECT max(timestamp_UTC) FROM event_summaries")
last_event_timestamp
last_event_timestamp=run_query("MachineLearning","SELECT max(timestamp_UTC) FROM event_summaries")[1,1]
last_event_timestamp
last_event_timestamp=run_query("MachineLearning","SELECT max(timestamp_UTC) FROM event_summaries")
unlist(last_event_timestamp)
last_event_timestamp=unlist(run_query("MachineLearning","SELECT max(timestamp_UTC) FROM event_summaries"))
last_event_timestamp
build()
load_all()
sensor_id=sensor_ids[1]
sensor_ids=unique(site_sensors$sensor_id)
sensor_id=sensor_ids[[1]]
raw_date_clause=sprintf("WHERE timestamp > %s",since_date)
since_date=last_event_timestamp
raw_date_clause=sprintf("WHERE timestamp > %s",since_date)
raw_query=sprintf("SELECT DATE(CONVERT_TZ(timestamp,'UTC','%s')) as local_date, timestamp as timestamp_UTC, pressure, accx, accy, accz, chipTemp FROM %s_Data %s",local_timezone,sensor_id,raw_date_clause)
raw = run_query("SweetDatabase",raw_query)
raw_date_clause
raw_date_clause=sprintf("WHERE timestamp > '%s'",since_date)
raw_query=sprintf("SELECT DATE(CONVERT_TZ(timestamp,'UTC','%s')) as local_date, timestamp as timestamp_UTC, pressure, accx, accy, accz, chipTemp FROM %s_Data %s",local_timezone,sensor_id,raw_date_clause)
raw = run_query("SweetDatabase",raw_query)
raw
build()
install()
document()
build()
install()
load_all()
raw_date_clause=sprintf("WHERE timestamp > '%s'",since_date)
raw_date_clause
raw_query=sprintf("SELECT DATE(CONVERT_TZ(timestamp,'UTC','%s')) as local_date, timestamp as timestamp_UTC, pressure, accx, accy, accz, chipTemp FROM %s_Data %s",local_timezone,sensor_id,raw_date_clause)
raw = run_query("SweetDatabase",raw_query)
raw[,timestamp_UTC:=as.POSIXct(strptime(timestamp_UTC,"%Y-%m-%d %H:%M:%S",tz="UTC"))]
raw
since_date
raw=raw[!duplicated(timestamp_UTC)]
events=run_query("SweetAnalysis", sprintf('SELECT * FROM %s_Events',sensor_id))
events=run_query("SweetAnalysis", sprintf('SELECT * FROM %s_Events %s',sensor_id,event_date_clause))
event_date_clause=sprintf("WHERE start_UTC > '%s'",since_date)
events=run_query("SweetAnalysis", sprintf('SELECT * FROM %s_Events %s',sensor_id,event_date_clause))
as.numeric(since_date)
since_date
as.POSIXct(since_date)
as.numeric(as.POSIXct(since_date))
event_date_clause=sprintf("WHERE starts > '%s'",as.numeric(as.POSIXct(since_date)))
events=run_query("SweetAnalysis", sprintf('SELECT * FROM %s_Events %s',sensor_id,event_date_clause))
events[,start_UTC:=as.POSIXct(starts,origin="1970-01-01",tz="UTC")]
events[,end_UTC:=as.POSIXct(ends,origin="1970-01-01",tz="UTC")]
events[,event_num:=order(starts)]
build()
install()
load_all()
last_event_timestamp=unlist(run_query("MachineLearning","SELECT max(timestamp_UTC) FROM event_summaries"))
event_data=get_event_data(site_sensors,last_event_timestamp)
sensor_ids=sensor_ids[[2]]
message(sensor_id)
########################
# pull raw sensor data
message("Pulling Raw Data")
raw_query=sprintf("SELECT DATE(CONVERT_TZ(timestamp,'UTC','%s')) as local_date, timestamp as timestamp_UTC, pressure, accx, accy, accz, chipTemp FROM %s_Data %s",local_timezone,sensor_id,raw_date_clause)
raw = run_query("SweetDatabase",raw_query)
raw[,timestamp_UTC:=as.POSIXct(strptime(timestamp_UTC,"%Y-%m-%d %H:%M:%S",tz="UTC"))]
raw=raw[!duplicated(timestamp_UTC)]
########################
# pull algorithm generated pump events
message("Pulling Pump Events")
events=run_query("SweetAnalysis", sprintf('SELECT * FROM %s_Events %s',sensor_id,event_date_clause))
events[,start_UTC:=as.POSIXct(starts,origin="1970-01-01",tz="UTC")]
events[,end_UTC:=as.POSIXct(ends,origin="1970-01-01",tz="UTC")]
events[,event_num:=order(starts)]
message("Combining Pump Events and Raw Data")
event_map=events[,list(event_start_UTC=start_UTC,timestamp_UTC=seq(from=start_UTC,to=end_UTC,by="1 sec")),by=list(event_num)]
event_data=merge(event_map,raw,by="timestamp_UTC")
event_data[,sensor_id:=sensor_id]
since_datre
since_date
last_event_timestamp
sensor_ids=sensor_ids[[2]]
sensor_ids
sensor_ids=unique(site_sensors$sensor_id)
sensor_id=sensor_ids[[2]]
message(sensor_id)
########################
# pull raw sensor data
message("Pulling Raw Data")
raw_query=sprintf("SELECT DATE(CONVERT_TZ(timestamp,'UTC','%s')) as local_date, timestamp as timestamp_UTC, pressure, accx, accy, accz, chipTemp FROM %s_Data %s",local_timezone,sensor_id,raw_date_clause)
raw = run_query("SweetDatabase",raw_query)
raw[,timestamp_UTC:=as.POSIXct(strptime(timestamp_UTC,"%Y-%m-%d %H:%M:%S",tz="UTC"))]
#remove duplicated pump data
raw=raw[!duplicated(timestamp_UTC)]
########################
# pull algorithm generated pump events
message("Pulling Pump Events")
events=run_query("SweetAnalysis", sprintf('SELECT * FROM %s_Events %s',sensor_id,event_date_clause))
events[,start_UTC:=as.POSIXct(starts,origin="1970-01-01",tz="UTC")]
events[,end_UTC:=as.POSIXct(ends,origin="1970-01-01",tz="UTC")]
events[,event_num:=order(starts)]
########################
#associate raw data with sweetsense detected events
message("Combining Pump Events and Raw Data")
#map raw readings into events
event_map=events[,list(event_start_UTC=start_UTC,timestamp_UTC=seq(from=start_UTC,to=end_UTC,by="1 sec")),by=list(event_num)]
event_data=merge(event_map,raw,by="timestamp_UTC")
events=run_query("SweetAnalysis", sprintf('SELECT * FROM %s_Events %s',sensor_id,event_date_clause))
events
events=run_query("SweetAnalysis", sprintf('SELECT * FROM %s_Events %s',sensor_id,event_date_clause))
events
events[,event_num:=starts]
events
build()
install()
load_all()
new_event_data=get_event_data(site_sensors,last_event_timestamp)
event_sum=generate_event_summaries(new_event_data)
append_table("MachineLearning","event_summaries",new_event_sum)
new_event_sum=generate_event_summaries(new_event_data)
append_table("MachineLearning","event_summaries",new_event_sum)
event_sum=run_query("MachineLearning","SELECT * FROM event_summaries")
streaming_event_sum=event_sum
last_event_timestamp=NULL
new_event_data=get_event_data(site_sensors,last_event_timestamp)
new_event_sum=generate_event_summaries(new_event_data)
write_table("MachineLearning","event_summaries",new_event_sum)
event_sum=run_query("MachineLearning","SELECT * FROM event_summaries")
setdifff(names(event_sum),names(new_event_sum))
setdiff(names(event_sum),names(new_event_sum))
write_table
?dbWriteTable
event_sum[row_names:=NULL]
str(event_sum)
event_sum[,row_names:=NULL]
all.equal(event_sum,new_event_sum)
head(event_sum)
event_ids=paste(event_sum$sensor_id,event_sum$event_num)
streaming_event_ids=paste(streaming_event_sum$sensor_id,streaming_event_sum$event_num)
table(streaming_event_ids)
table(table(streaming_event_ids))
table(table(event_ids))
new_event_sum=new_event_sum[timestamp_UTC<"2017-01-31"]
write_table("MachineLearning","event_summaries",new_event_sum)
last_event_timestamp=unlist(run_query("MachineLearning","SELECT max(timestamp_UTC) FROM event_summaries"))
last_event_timestamp
new_event_data=get_event_data(site_sensors,last_event_timestamp)
new_event_sum=generate_event_summaries(new_event_data)
event_sum=run_query("MachineLearning","SELECT * FROM event_summaries")
dim(event_sum)
dim(new_event_sum)
dim(event_sum)+dim(new_event_sum)
append_table("MachineLearning","event_summaries",new_event_sum)
event_sum=run_query("MachineLearning","SELECT * FROM event_summaries")
event_ids2=paste(event_sum$sensor_id,event_sum$event_num)
setdiff(event_ids,event_ids2)
missing_events=setdiff(event_ids,event_ids2)
missing_event_times=gsub("._ ","",missing_events)
missing_event_times
missing_event_times=gsub(".* ","",missing_events)
missing_event_times
missing_event_times=as.numeric(gsub(".* ","",missing_events))
missing_event_times=as.POSIXct(missing_event_times,origin="1970-01-01")
missing_event_times
missing_event_times=sort(as.POSIXct(missing_event_times,origin="1970-01-01"))
missing_event_times
event_date_clause=sprintf("WHERE starts > '%s'",as.numeric(as.POSIXct(since_date,tz="UTC")))
event_date_clause
event_date_clause=sprintf("WHERE starts > '%s'",as.numeric(as.POSIXct(since_date)))
event_date_clause
site_sensors=map_site_sensors()
write_table("MachineLearning","site_sensors",site_sensors)
#get sensor data, separated into discrete pumping events
# last_event_timestamp=unlist(run_query("MachineLearning","SELECT max(timestamp_UTC) FROM event_summaries"))
# last_event_timestamp=NULL
new_event_data=get_event_data(site_sensors,last_event_timestamp)
#create event level summaries
new_event_sum=generate_event_summaries(new_event_data)
write_table("MachineLearning","event_summaries",new_event_sum)
event_sum=run_query("MachineLearning","SELECT * FROM event_summaries")
last_event_timestamp
last_event_timestamp
last_event_timestamp=NULL
new_event_data=get_event_data(site_sensors,last_event_timestamp)
#create event level summaries
new_event_sum=generate_event_summaries(new_event_data)
write_table("MachineLearning","event_summaries",new_event_sum)
event_sum=run_query("MachineLearning","SELECT * FROM event_summaries")
full_event_ids=paste(event_sum$sensor_id,event_sum$event_num)
new_event_sum=new_event_sum[timestamp_UTC<"2017-01-31"]
write_table("MachineLearning","event_summaries",new_event_sum)
last_event_timestamp=unlist(run_query("MachineLearning","SELECT max(timestamp_UTC) FROM event_summaries"))
new_event_data=get_event_data(site_sensors,last_event_timestamp)
new_event_sum=generate_event_summaries(new_event_data)
append_table("MachineLearning","event_summaries",new_event_sum)
event_sum=run_query("MachineLearning","SELECT * FROM event_summaries")
streaming_event_ids=paste(event_sum$sensor_id,event_sum$event_num)
missing_events=setdiff(full_event_ids,streaming_event_ids)
missing_events
get_event_data
build()
install()
load_all()
new_event_data=get_event_data(site_sensors,last_event_timestamp)
#create event level summaries
new_event_sum=generate_event_summaries(new_event_data)
5513-5428
new_streaming_event_ids=paste(new_event_sum$sensor_id,new_event_sum$event_num)
streaming_event_ids2=union(streaming_event_ids,new_streaming_event_ids)
missing_events=setdiff(full_event_ids,streaming_event_ids2)
missing_events
missing_events=setdiff(streaming_event_ids2,full_event_ids)
missing_events
missing_event_times=as.numeric(gsub(".* ","",missing_events))
missing_event_times=sort(as.POSIXct(missing_event_times,origin="1970-01-01"))
missing_event_times
data=new_event_sum
db_name="MachineLearning"
table="event_summaries"
keys=c("sensor_id","event_num")
existing_query=sprintf("SELECT DISTINCT %s FROM %s",paste(keys,collapse=", "),table)
existing_query
existing=run_query(db_name,existing_query)
keydata=data[,keys,with=F]
keydata
keystrs=apply(keydata,1,paste,collapse=", ")
keystrs
keystr=sprintf("((%s))",paste(keystrs,collapse=") , ("))
keystr
keynamestr=sprintf("(%s)",paste(keys,collapse=", "))
keynamestr
delete_query=sprintf("DELETE FROM %s WHERE %s in %s",table, keynamestr,keystr)
delete_query
existing=run_query(db_name,delete_query)
delete_query
keydata=data[1:5,keys,with=F]
keystrs=apply(keydata,1,paste,collapse=", ")
keystr=sprintf("((%s))",paste(keystrs,collapse=") , ("))
keystr
keynamestr=sprintf("(%s)",paste(keys,collapse=", "))
delete_query=sprintf("DELETE FROM %s WHERE %s in %s",table, keynamestr,keystr)
delete_query
n = 200
Qbar = function(A,W){A*W}
blip = function(W){Qbar(1,W)-Qbar(0,W)}
# validation set
Wval = rnorm(1e5)
out = sapply(1:100,function(i){
if(i%%10==0) print(paste0('Repetition ',i))
W = rnorm(n)
A = rbinom(n,1,1/2)
Y = rnorm(n,mean=Qbar(A,W))
Q.SL = SuperLearner(Y,data.frame(A=A,W=W),SL.library=c('SL.randomForest','SL.rpart'),newX=data.frame(A=c(rep(0,n),rep(1,n)),W=W))
pred0 = head(Q.SL$SL.predict,n)
pred1 = tail(Q.SL$SL.predict,n)
pseudoY = (2*A-1)/2 * (Y-A*pred1-(1-A)*pred0) + pred1 - pred0
blip.SL = SuperLearner(pseudoY,data.frame(W=W),SL.library=c('SL.mean','SL.glm'),newX=data.frame(W=Wval))
mse.full = mean((blip.SL$SL.predict-blip(Wval))^2)
n.2 = floor(n*0.9)
W.2 = rnorm(n.2)
A.2 = rbinom(n.2,1,1/2)
Y.2 = rnorm(n.2,mean=Qbar(A.2,W.2))
pred0.2 = predict(Q.SL,newdata=data.frame(A=0,W=W.2))$pred
pred1.2 = predict(Q.SL,newdata=data.frame(A=1,W=W.2))$pred
pseudoY.2 = (2*A.2-1)/2 * (Y.2-A.2*pred1.2-(1-A.2)*pred0.2) + pred1.2 - pred0.2
blip.SL2 = SuperLearner(pseudoY.2,data.frame(W=W.2),SL.library=c('SL.mean','SL.glm'),newX=data.frame(W=Wval))
mse.split = mean((blip.SL2$SL.predict-blip(Wval))^2)
return(c(mse.full,mse.split))
})
rowMeans(out)
n = 200
Qbar = function(A,W){A*W}
blip = function(W){Qbar(1,W)-Qbar(0,W)}
# validation set
Wval = rnorm(1e5)
out = sapply(1:100,function(i){
if(i%%10==0) print(paste0('Repetition ',i))
W = rnorm(n)
A = rbinom(n,1,1/2)
Y = rnorm(n,mean=Qbar(A,W))
Q.SL = SuperLearner(Y,data.frame(A=A,W=W),SL.library=c('SL.randomForest','SL.rpart'),newX=data.frame(A=c(rep(0,n),rep(1,n)),W=W))
pred0 = head(Q.SL$SL.predict,n)
pred1 = tail(Q.SL$SL.predict,n)
pseudoY = (2*A-1)/2 * (Y-A*pred1-(1-A)*pred0) + pred1 - pred0
blip.SL = SuperLearner(pseudoY,data.frame(W=W),SL.library=c('SL.mean','SL.glm'),newX=data.frame(W=Wval))
mse.full = mean((blip.SL$SL.predict-blip(Wval))^2)
n.2 = floor(n*0.9)
W.2 = rnorm(n.2)
A.2 = rbinom(n.2,1,1/2)
Y.2 = rnorm(n.2,mean=Qbar(A.2,W.2))
pred0.2 = predict(Q.SL,newdata=data.frame(A=0,W=W.2))$pred
pred1.2 = predict(Q.SL,newdata=data.frame(A=1,W=W.2))$pred
pseudoY.2 = (2*A.2-1)/2 * (Y.2-A.2*pred1.2-(1-A.2)*pred0.2) + pred1.2 - pred0.2
blip.SL2 = SuperLearner(pseudoY.2,data.frame(W=W.2),SL.library=c('SL.mean','SL.glm'),newX=data.frame(W=Wval))
mse.split = mean((blip.SL2$SL.predict-blip(Wval))^2)
return(c(mse.full,mse.split))
})
rowMeans(out)
library(origami)
library(reshape2)
setwd("~/Dropbox/thesis/simulations/opttx_nesting_sim/")
source("dodgev.R")
files <- dir(path = "opttx_nesting_results_200", full.names = T)
B <- length(files)
print(B)
allresults <- lapply(files, function(file) {
load(file)
results
})
# load('simresults/nesting_sim_all.rdata')
results <- apply(do.call(rbind, allresults), 2, as.list)
combined <- combine_results(results)
alist <- combined$times[[1]]
listtodf <- function(alist) {
df <- data.frame(do.call(rbind, alist))
df$condition <- rownames(df)
df
}
# run times
conditions <- c("splitrule", "fullrule", "nestrule", "tmle", "cvtmle", "nestcvtmle")
goodconditions <- c("SplitSequential", "FullSequential", "Nested",
"Canonical  TMLE", "SplitSequential CV-TMLE", "Nested CV-TMLE")
times <- ldply(combined$times, listtodf)
times$iteration <- rep(seq_len(B), each = nrow(times)/B)
times$goodcondition <- goodconditions[match(times$condition, conditions)]
times$goodcondition <- factor(times$goodcondition, goodconditions)
# rule performance
perfs <- melt(combined$test_perf,id=c("iteration","rule_fit"),variable.name ="measure")
perfs$condition <- perfs$rule_fit
wald=function(x){
est=mean(x)
se=sd(x)/sqrt(length(x))
lower=est-qnorm(0.975)*se
upper=est+qnorm(0.975)*se
c(est=est,se=se,lower=lower,upper=upper)
}
head(perfs)
ggplot(perfs,aes(x=rule_fit,y=value))+geom_boxplot()+facet_wrap(~measure,scales="free")+theme_bw()
aggwald=aggregate(value~rule_fit+measure,perfs,wald)
aggwald=cbind(aggwald,aggwald$value)
aggwald$cv=aggwald$measure%in%c("cv_EYdn","cv_mse")
aggwald$measure=gsub("cv_","",aggwald$measure)
# dodge=position_dodgev(height=0.5)
library(ggplot2)
ggplot(perfs,aes(x=rule_fit,y=value))+geom_boxplot()+facet_wrap(~measure,scales="free")+theme_bw()
ggplot(aggwald[!aggwald$cv,],aes(y=goodfit,x=est,xmin=lower,xmax=upper))+
geom_point()+geom_errorbarh()+
facet_wrap(~measure,scales="free_x")+theme_bw()+xlab("Performance")+ylab("Nesting Approach")+theme_bw()+
geom_vline(aes(xintercept=x),data=data.frame(measure="EYdn",x=0.5628),linetype="dashed")
aggwald$goodfit=factor(aggwald$rule_fit,levels=c("split_rule_fit","full_rule_fit","nested_rule_fit"),labels=c("SplitSequential","FullSequential","Nested"))
ggplot(aggwald[!aggwald$cv,],aes(y=goodfit,x=est,xmin=lower,xmax=upper))+
geom_point()+geom_errorbarh()+
facet_wrap(~measure,scales="free_x")+theme_bw()+xlab("Performance")+ylab("Nesting Approach")+theme_bw()+
geom_vline(aes(xintercept=x),data=data.frame(measure="EYdn",x=0.5628),linetype="dashed")
library(origami)
origami_SuperLearner
origami:::predict.origami_SuperLearner_fit
SL.mean
predict.SuperLearner
predict.origami_SuperLearner
origami:::predict.origami_SuperLearner
method.NNLS
SL.glmnet
?SuperLearner
predict.SL.glmnet
mnSL.earth()
mnSL.earth
setwd("~/Dropbox/origami/")
build()
install()
load_all()
library(opttx)
?origami_SuperLearner
sl_result <- SuperLearner(Y = Y, X = X, SL.library = SL.library, method = method.NNLS(), family = binomial(), cvControl = list(validRows = vfolds))
sl_result
SL.glmnet
predict.SL.glmnet
use_tests()
uses_testthat()
use_testthat()
SL.mean
SL.arg_wrapper <- function (Y, X, newX, family, obsWeights, id, fake_mean=0, ...)
{
# meanY <- weighted.mean(Y, w = obsWeights)
meanY <- fake_mean
pred <- rep.int(meanY, times = nrow(newX))
fit <- list(object = meanY)
out <- list(pred = pred, fit = fit)
class(out$fit) <- c("SL.arg_wrapper")
return(out)
}
predict.SL.mean
context("Wrapper Args")
#wrapper that uses dot args for prediction
#modified from SL.mean
SL.arg_wrapper <- function (Y, X, newX, family, obsWeights, id, fake_mean=0, ...)
{
# meanY <- weighted.mean(Y, w = obsWeights)
meanY <- fake_mean
pred <- rep.int(meanY, times = nrow(newX))
fit <- list(object = meanY)
out <- list(pred = pred, fit = fit)
class(out$fit) <- c("SL.arg_wrapper")
return(out)
}
predict.SL.arg_wrapper <- function (object, newdata, family, X = NULL, Y = NULL, fake_mean=NULL, ...)
{
if(is.null(fake_mean)){
meanY=object$object
} else {
meanY=fake_mean
}
pred <- rep.int(meanY, times = nrow(newdata))
return(pred)
}
set.seed(1)
set.seed(1)
N <- 200
p <- 6
X <- matrix(rnorm(N * p), N, p)
X <- as.data.frame(X)
Y <- rbinom(N, 1, plogis(0.2 * X[, 1] + 0.1 * X[, 2] - 0.2 * X[, 3] + 0.1 * X[, 3] * X[, 4] - 0.2 * abs(X[, 4])))
SL.library <- "SL.fake_mean"
osl_result <- origami_SuperLearner(Y = Y, X = X, SL.library = SL.library, method = method.NNLS(), family = binomial(), folds = folds)
osl_result <- origami_SuperLearner(Y = Y, X = X, SL.library = SL.library, method = method.NNLS(), family = binomial())
SL.library <- "SL.arg_wrapper"
osl_result <- origami_SuperLearner(Y = Y, X = X, SL.library = SL.library, method = method.NNLS(), family = binomial())
osl_result
#wrapper that uses dot args for prediction
#modified from SL.mean
SL.arg_wrapper <- function (Y, X, newX, family, obsWeights, id, ...)
{
meanY <- weighted.mean(Y, w = obsWeights)
pred <- rep.int(meanY, times = nrow(newX))
fit <- list(object = meanY)
out <- list(pred = pred, fit = fit)
class(out$fit) <- c("SL.arg_wrapper")
return(out)
}
predict.SL.arg_wrapper <- function (object, newdata, family, X = NULL, Y = NULL, fake_mean=NULL, ...)
{
if(is.null(fake_mean)){
meanY <- object$object
} else {
meanY <- fake_mean
}
pred <- rep.int(meanY, times = nrow(newdata))
return(pred)
}
set.seed(1)
N <- 200
p <- 6
X <- matrix(rnorm(N * p), N, p)
X <- as.data.frame(X)
Y <- rbinom(N, 1, plogis(0.2 * X[, 1] + 0.1 * X[, 2] - 0.2 * X[, 3] + 0.1 * X[, 3] * X[, 4] - 0.2 * abs(X[, 4])))
SL.library <- "SL.arg_wrapper"
osl_result <- origami_SuperLearner(Y = Y, X = X, SL.library = SL.library, method = method.NNLS(), family = binomial())
osl_result
osl_result <- origami_SuperLearner(Y = Y, X = X, SL.library = SL.library, method = method.NNLS(), family = binomial())
predict(osl_result,newdata=X)
meanY=mean(Y)
all(predict(osl_result,newdata=X)==meanY)
predict(osl_result,newdata=X)
all(predict(osl_result,newdata=X)$pred==meanY)
?expect_true
?expect_equal
library(testthat)
?expect_true
?expect_equal
mean_preds=rep.int(meanY, times = nrow(X))
test_that(expect_equal(predict(osl_result,newdata=X)$pred,mean_preds))
expected_preds=rep.int(meanY, times = nrow(X))
mean_preds <- predict(osl_result,newdata=X)$pred
expected_preds <- rep.int(meanY, times = nrow(X))
mean_preds
expected_preds
expected_preds <- matrix(meanY, nrow = nrow(X),ncol=1)
expected_preds
expect_equal(mean_preds,expected_preds)
test_that("SL.arg_wrapper works without arguments passed",expect_equal(mean_preds,expected_preds))
fake_mean_preds <- predict(osl_result,newdata=X,fake_mean=0)$pred
expected_fake_preds <- matrix(0, nrow = nrow(X),ncol=1)
fake_mean_preds
expected_fake_preds
test_that("SL.arg_wrapper works with arguments passed",expect_equal(fake_mean_preds,expected_fake_preds))
predict.origami_SuperLearner()
predict.origami_SuperLearner
build()
install()
load_all()
fake_mean_preds <- predict(osl_result,newdata=X,fake_mean=0)$pred
fake_mean_preds
expected_fake_preds <- matrix(0, nrow = nrow(X),ncol=1)
test_that("SL.arg_wrapper works with arguments passed",expect_equal(fake_mean_preds,expected_fake_preds))
test()
build()
install()
load_all()
build()
install()
load_all()
9.8/12
9.8/12*30
db_name
keydata
keystrs
keystrs=apply(keydata,1,paste,collapse=", ")
keystr=sprintf("((%s))",paste(keystrs,collapse=") , ("))
keynamestr=sprintf("(%s)",paste(keys,collapse=", "))
install.packages("dplyr")
?dplyr
library(dplyr)
?dplyr
install.packages("RODBC")
library(RODBC)
